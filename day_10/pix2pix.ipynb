{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq8LRzhhOg23",
        "outputId": "63e7b6c8-9eba-4b61-f765-8d87ef037f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.15.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.6.11)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install matplotlib\n",
        "!pip install scikit-image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports\n"
      ],
      "metadata": {
        "id": "fQyRJ2KK98cI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "AC9g_ZEIP_rY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpConvBlock(nn.Module):\n",
        "    def __init__(self, ip_sz, op_sz, dropout=0.0):\n",
        "        super(UpConvBlock, self).__init__()\n",
        "        self.layers = [\n",
        "            nn.ConvTranspose2d(ip_sz, op_sz, 4, 2, 1),\n",
        "            nn.InstanceNorm2d(op_sz),\n",
        "            nn.ReLU(),\n",
        "        ]\n",
        "        if dropout:\n",
        "            self.layers += [nn.Dropout(dropout)]\n",
        "    def forward(self, x, enc_ip):\n",
        "        x = nn.Sequential(*(self.layers))(x)\n",
        "        op = torch.cat((x, enc_ip), 1)\n",
        "        return op"
      ],
      "metadata": {
        "id": "OFReZ2JyP3ga"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DownConvBlock(nn.Module):\n",
        "    def __init__(self, ip_sz, op_sz, norm=True, dropout=0.0):\n",
        "        super(DownConvBlock, self).__init__()\n",
        "        self.layers = [nn.Conv2d(ip_sz, op_sz, 4, 2, 1)]\n",
        "        if norm:\n",
        "            self.layers.append(nn.InstanceNorm2d(op_sz))\n",
        "        self.layers += [nn.LeakyReLU(0.2)]\n",
        "        if dropout:\n",
        "            self.layers += [nn.Dropout(dropout)]\n",
        "    def forward(self, x):\n",
        "        op = nn.Sequential(*(self.layers))(x)\n",
        "        return op"
      ],
      "metadata": {
        "id": "JiSsbD8_QPil"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the U-Net based Generator\n"
      ],
      "metadata": {
        "id": "RXOsaxnW-BQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNetGenerator(nn.Module):\n",
        "    def __init__(self, chnls_in=3, chnls_op=3):\n",
        "        super(UNetGenerator, self).__init__()\n",
        "        self.down_conv_layer_1 = DownConvBlock(chnls_in, 64, norm=False)\n",
        "        self.down_conv_layer_2 = DownConvBlock(64, 128)\n",
        "        self.down_conv_layer_3 = DownConvBlock(128, 256)\n",
        "        self.down_conv_layer_4 = DownConvBlock(256, 512, dropout=0.5)\n",
        "        self.down_conv_layer_5 = DownConvBlock(512, 512, dropout=0.5)\n",
        "        self.down_conv_layer_6 = DownConvBlock(512, 512, dropout=0.5)\n",
        "        self.down_conv_layer_7 = DownConvBlock(512, 512, dropout=0.5)\n",
        "        self.down_conv_layer_8 = DownConvBlock(512, 512, norm=False, dropout=0.5)\n",
        "        self.up_conv_layer_1 = UpConvBlock(512, 512, dropout=0.5)\n",
        "        self.up_conv_layer_2 = UpConvBlock(1024, 512, dropout=0.5)\n",
        "        self.up_conv_layer_3 = UpConvBlock(1024, 512, dropout=0.5)\n",
        "        self.up_conv_layer_4 = UpConvBlock(1024, 512, dropout=0.5)\n",
        "        self.up_conv_layer_5 = UpConvBlock(1024, 256)\n",
        "        self.up_conv_layer_6 = UpConvBlock(512, 128)\n",
        "        self.up_conv_layer_7 = UpConvBlock(256, 64)\n",
        "        self.upsample_layer = nn.Upsample(scale_factor=2)\n",
        "        self.zero_pad = nn.ZeroPad2d((1, 0, 1, 0))\n",
        "        self.conv_layer_1 = nn.Conv2d(128, chnls_op, 4, padding=1)\n",
        "        self.activation = nn.Tanh()\n",
        "    def forward(self, x):\n",
        "        enc1 = self.down_conv_layer_1(x)\n",
        "        enc2 = self.down_conv_layer_2(enc1)\n",
        "        enc3 = self.down_conv_layer_3(enc2)\n",
        "        enc4 = self.down_conv_layer_4(enc3)\n",
        "        enc5 = self.down_conv_layer_5(enc4)\n",
        "        enc6 = self.down_conv_layer_6(enc5)\n",
        "        enc7 = self.down_conv_layer_7(enc6)\n",
        "        enc8 = self.down_conv_layer_8(enc7)\n",
        "        dec1 = self.up_conv_layer_1(enc8, enc7)\n",
        "        dec2 = self.up_conv_layer_2(dec1, enc6)\n",
        "        dec3 = self.up_conv_layer_3(dec2, enc5)\n",
        "        dec4 = self.up_conv_layer_4(dec3, enc4)\n",
        "        dec5 = self.up_conv_layer_5(dec4, enc3)\n",
        "        dec6 = self.up_conv_layer_6(dec5, enc2)\n",
        "        dec7 = self.up_conv_layer_7(dec6, enc1)\n",
        "        final = self.upsample_layer(dec7)\n",
        "        final = self.zero_pad(final)\n",
        "        final = self.conv_layer_1(final)\n",
        "        return self.activation(final)"
      ],
      "metadata": {
        "id": "gPBecIwsRRpJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the Discriminator\n"
      ],
      "metadata": {
        "id": "njysPTXqQEW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Pix2PixDiscriminator(nn.Module):\n",
        "    def __init__(self, chnls_in=3):\n",
        "        super(Pix2PixDiscriminator, self).__init__()\n",
        "        def disc_conv_block(chnls_in, chnls_op, norm=1):\n",
        "            layers = [nn.Conv2d(chnls_in, chnls_op, 4, stride=2, padding=1)]\n",
        "            if normalization:\n",
        "                layers.append(nn.InstanceNorm2d(chnls_op))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "        self.lyr1 = disc_conv_block(chnls_in * 2, 64, norm=0)\n",
        "        self.lyr2 = disc_conv_block(64, 128)\n",
        "        self.lyr3 = disc_conv_block(128, 256)\n",
        "        self.lyr4 = disc_conv_block(256, 512)\n",
        "    def forward(self, real_image, translated_image):\n",
        "        ip = torch.cat((real_image, translated_image), 1)\n",
        "        op = self.lyr1(ip)\n",
        "        op = self.lyr2(op)\n",
        "        op = self.lyr3(op)\n",
        "        op = self.lyr4(op)\n",
        "        op = nn.ZeroPad2d((1, 0, 1, 0))(op)\n",
        "        op = nn.Conv2d(512, 1, 4, padding=1)(op)\n",
        "        return op"
      ],
      "metadata": {
        "id": "VEQRdt5IQB1p"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkJOAUEvQjn8",
        "outputId": "fda66a34-67ab-4159-db25-9796db6afe40"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.13)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import io\n",
        "\n",
        "# Your model classes (with fixed discriminator)\n",
        "class UpConvBlock(nn.Module):\n",
        "    def __init__(self, ip_sz, op_sz, dropout=0.0):\n",
        "        super(UpConvBlock, self).__init__()\n",
        "        self.layers = [\n",
        "            nn.ConvTranspose2d(ip_sz, op_sz, 4, 2, 1),\n",
        "            nn.InstanceNorm2d(op_sz),\n",
        "            nn.ReLU(),\n",
        "        ]\n",
        "        if dropout:\n",
        "            self.layers += [nn.Dropout(dropout)]\n",
        "\n",
        "    def forward(self, x, enc_ip):\n",
        "        x = nn.Sequential(*(self.layers))(x)\n",
        "        op = torch.cat((x, enc_ip), 1)\n",
        "        return op\n",
        "\n",
        "class DownConvBlock(nn.Module):\n",
        "    def __init__(self, ip_sz, op_sz, norm=True, dropout=0.0):\n",
        "        super(DownConvBlock, self).__init__()\n",
        "        self.layers = [nn.Conv2d(ip_sz, op_sz, 4, 2, 1)]\n",
        "        if norm:\n",
        "            self.layers.append(nn.InstanceNorm2d(op_sz))\n",
        "        self.layers += [nn.LeakyReLU(0.2)]\n",
        "        if dropout:\n",
        "            self.layers += [nn.Dropout(dropout)]\n",
        "\n",
        "    def forward(self, x):\n",
        "        op = nn.Sequential(*(self.layers))(x)\n",
        "        return op\n",
        "\n",
        "class UNetGenerator(nn.Module):\n",
        "    def __init__(self, chnls_in=3, chnls_op=3):\n",
        "        super(UNetGenerator, self).__init__()\n",
        "        self.down_conv_layer_1 = DownConvBlock(chnls_in, 64, norm=False)\n",
        "        self.down_conv_layer_2 = DownConvBlock(64, 128)\n",
        "        self.down_conv_layer_3 = DownConvBlock(128, 256)\n",
        "        self.down_conv_layer_4 = DownConvBlock(256, 512, dropout=0.5)\n",
        "        self.down_conv_layer_5 = DownConvBlock(512, 512, dropout=0.5)\n",
        "        self.down_conv_layer_6 = DownConvBlock(512, 512, dropout=0.5)\n",
        "        self.down_conv_layer_7 = DownConvBlock(512, 512, dropout=0.5)\n",
        "        self.down_conv_layer_8 = DownConvBlock(512, 512, norm=False, dropout=0.5)\n",
        "        self.up_conv_layer_1 = UpConvBlock(512, 512, dropout=0.5)\n",
        "        self.up_conv_layer_2 = UpConvBlock(1024, 512, dropout=0.5)\n",
        "        self.up_conv_layer_3 = UpConvBlock(1024, 512, dropout=0.5)\n",
        "        self.up_conv_layer_4 = UpConvBlock(1024, 512, dropout=0.5)\n",
        "        self.up_conv_layer_5 = UpConvBlock(1024, 256)\n",
        "        self.up_conv_layer_6 = UpConvBlock(512, 128)\n",
        "        self.up_conv_layer_7 = UpConvBlock(256, 64)\n",
        "        self.upsample_layer = nn.Upsample(scale_factor=2)\n",
        "        self.zero_pad = nn.ZeroPad2d((1, 0, 1, 0))\n",
        "        self.conv_layer_1 = nn.Conv2d(128, chnls_op, 4, padding=1)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.down_conv_layer_1(x)\n",
        "        enc2 = self.down_conv_layer_2(enc1)\n",
        "        enc3 = self.down_conv_layer_3(enc2)\n",
        "        enc4 = self.down_conv_layer_4(enc3)\n",
        "        enc5 = self.down_conv_layer_5(enc4)\n",
        "        enc6 = self.down_conv_layer_6(enc5)\n",
        "        enc7 = self.down_conv_layer_7(enc6)\n",
        "        enc8 = self.down_conv_layer_8(enc7)\n",
        "        dec1 = self.up_conv_layer_1(enc8, enc7)\n",
        "        dec2 = self.up_conv_layer_2(dec1, enc6)\n",
        "        dec3 = self.up_conv_layer_3(dec2, enc5)\n",
        "        dec4 = self.up_conv_layer_4(dec3, enc4)\n",
        "        dec5 = self.up_conv_layer_5(dec4, enc3)\n",
        "        dec6 = self.up_conv_layer_6(dec5, enc2)\n",
        "        dec7 = self.up_conv_layer_7(dec6, enc1)\n",
        "        final = self.upsample_layer(dec7)\n",
        "        final = self.zero_pad(final)\n",
        "        final = self.conv_layer_1(final)\n",
        "        return self.activation(final)\n",
        "\n",
        "class Pix2PixDiscriminator(nn.Module):\n",
        "    def __init__(self, chnls_in=3):\n",
        "        super(Pix2PixDiscriminator, self).__init__()\n",
        "\n",
        "        def disc_conv_block(chnls_in, chnls_op, norm=True):\n",
        "            layers = [nn.Conv2d(chnls_in, chnls_op, 4, stride=2, padding=1)]\n",
        "            if norm:\n",
        "                layers.append(nn.InstanceNorm2d(chnls_op))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "        self.lyr1 = disc_conv_block(chnls_in * 2, 64, norm=False)\n",
        "        self.lyr2 = disc_conv_block(64, 128)\n",
        "        self.lyr3 = disc_conv_block(128, 256)\n",
        "        self.lyr4 = disc_conv_block(256, 512)\n",
        "        self.zero_pad = nn.ZeroPad2d((1, 0, 1, 0))\n",
        "        self.final_conv = nn.Conv2d(512, 1, 4, padding=1)\n",
        "\n",
        "    def forward(self, real_image, translated_image):\n",
        "        ip = torch.cat((real_image, translated_image), 1)\n",
        "        op = self.lyr1(ip)\n",
        "        op = self.lyr2(op)\n",
        "        op = self.lyr3(op)\n",
        "        op = self.lyr4(op)\n",
        "        op = self.zero_pad(op)\n",
        "        op = self.final_conv(op)\n",
        "        return op\n",
        "\n",
        "# Global model variable\n",
        "generator = None\n",
        "\n",
        "def load_model(model_file, model_type):\n",
        "    \"\"\"Load the Pix2Pix generator model\"\"\"\n",
        "    global generator\n",
        "    try:\n",
        "        generator = UNetGenerator(chnls_in=3, chnls_op=3)\n",
        "\n",
        "        if model_file is not None:\n",
        "            # Load custom model\n",
        "            model_state = torch.load(model_file.name, map_location='cpu')\n",
        "            generator.load_state_dict(model_state)\n",
        "            status = f\"✅ Custom model loaded successfully!\"\n",
        "        else:\n",
        "            # Use randomly initialized model for demo\n",
        "            status = f\"⚠️ Using randomly initialized model (upload a trained model for real results)\"\n",
        "\n",
        "        generator.eval()\n",
        "        return status\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error loading model: {str(e)}\"\n",
        "\n",
        "def transform_image(input_image, model_file, model_type, image_size):\n",
        "    \"\"\"Transform input image using Pix2Pix model\"\"\"\n",
        "    if input_image is None:\n",
        "        return None, \"Please upload an image first!\"\n",
        "\n",
        "    try:\n",
        "        # Load model if not already loaded or if new model uploaded\n",
        "        if generator is None or model_file is not None:\n",
        "            status = load_model(model_file, model_type)\n",
        "        else:\n",
        "            status = \"Using previously loaded model\"\n",
        "\n",
        "        # Preprocess image\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "        ])\n",
        "\n",
        "        # Convert PIL to tensor\n",
        "        input_tensor = transform(input_image).unsqueeze(0)\n",
        "\n",
        "        # Generate output\n",
        "        with torch.no_grad():\n",
        "            output_tensor = generator(input_tensor)\n",
        "\n",
        "        # Post-process output\n",
        "        output_tensor = (output_tensor + 1) / 2.0  # Denormalize from [-1,1] to [0,1]\n",
        "        output_tensor = torch.clamp(output_tensor, 0, 1)\n",
        "\n",
        "        # Convert back to PIL Image\n",
        "        output_image = transforms.ToPILImage()(output_tensor.squeeze(0))\n",
        "\n",
        "        return output_image, status\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"❌ Error processing image: {str(e)}\"\n",
        "\n",
        "def create_demo():\n",
        "    \"\"\"Create the Gradio interface\"\"\"\n",
        "\n",
        "    # Custom CSS for better styling\n",
        "    css = \"\"\"\n",
        "    .gradio-container {\n",
        "        font-family: 'Arial', sans-serif;\n",
        "    }\n",
        "    .gr-button-primary {\n",
        "        background: linear-gradient(45deg, #FF6B35, #F7931E) !important;\n",
        "        border: none !important;\n",
        "    }\n",
        "    .gr-button-primary:hover {\n",
        "        transform: scale(1.05) !important;\n",
        "        transition: all 0.2s !important;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    with gr.Blocks(css=css, title=\"Pix2Pix Image Translation\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            # 🎨 Pix2Pix Image Translation\n",
        "            Transform your images using conditional GANs! Upload an image and watch the magic happen ✨\n",
        "            \"\"\",\n",
        "            elem_id=\"header\"\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                # Input section\n",
        "                gr.Markdown(\"### 📤 Input\")\n",
        "                input_image = gr.Image(\n",
        "                    type=\"pil\",\n",
        "                    label=\"Upload your image\",\n",
        "                    height=300\n",
        "                )\n",
        "\n",
        "                # Model configuration\n",
        "                gr.Markdown(\"### ⚙️ Configuration\")\n",
        "                model_type = gr.Dropdown(\n",
        "                    choices=[\"Sketch to Photo\", \"Day to Night\", \"Satellite to Map\", \"Custom Model\"],\n",
        "                    value=\"Sketch to Photo\",\n",
        "                    label=\"Transformation Type\"\n",
        "                )\n",
        "\n",
        "                model_file = gr.File(\n",
        "                    label=\"Upload Custom Model (.pth/.pt)\",\n",
        "                    file_types=[\".pth\", \".pt\"],\n",
        "                    visible=False\n",
        "                )\n",
        "\n",
        "                image_size = gr.Slider(\n",
        "                    minimum=128,\n",
        "                    maximum=512,\n",
        "                    step=64,\n",
        "                    value=256,\n",
        "                    label=\"Image Size\"\n",
        "                )\n",
        "\n",
        "                # Transform button\n",
        "                transform_btn = gr.Button(\n",
        "                    \"🚀 Transform Image\",\n",
        "                    variant=\"primary\",\n",
        "                    size=\"lg\"\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                # Output section\n",
        "                gr.Markdown(\"### 📤 Output\")\n",
        "                output_image = gr.Image(\n",
        "                    label=\"Transformed Image\",\n",
        "                    height=300\n",
        "                )\n",
        "\n",
        "                status_text = gr.Textbox(\n",
        "                    label=\"Status\",\n",
        "                    interactive=False,\n",
        "                    max_lines=3\n",
        "                )\n",
        "\n",
        "        # Show/hide custom model upload based on selection\n",
        "        def update_model_visibility(choice):\n",
        "            return gr.update(visible=(choice == \"Custom Model\"))\n",
        "\n",
        "        model_type.change(\n",
        "            update_model_visibility,\n",
        "            inputs=[model_type],\n",
        "            outputs=[model_file]\n",
        "        )\n",
        "\n",
        "        # Transform button click event\n",
        "        transform_btn.click(\n",
        "            transform_image,\n",
        "            inputs=[input_image, model_file, model_type, image_size],\n",
        "            outputs=[output_image, status_text]\n",
        "        )\n",
        "\n",
        "        # Information tabs\n",
        "        with gr.Tabs():\n",
        "            with gr.Tab(\"ℹ️ About Pix2Pix\"):\n",
        "                gr.Markdown(\n",
        "                    \"\"\"\n",
        "                    **Pix2Pix** is a conditional Generative Adversarial Network (cGAN) that learns to map from input images to output images.\n",
        "\n",
        "                    ### How it works:\n",
        "                    - **U-Net Generator**: Uses skip connections to preserve fine details\n",
        "                    - **PatchGAN Discriminator**: Focuses on local image patches for realistic textures\n",
        "                    - **Loss Function**: Combines adversarial loss with L1 loss for pixel-level accuracy\n",
        "\n",
        "                    ### Common Applications:\n",
        "                    - 🎨 Sketch to photorealistic images\n",
        "                    - 🌅 Day to night scene conversion\n",
        "                    - 🗺️ Satellite imagery to maps\n",
        "                    - 🎨 Image colorization\n",
        "                    - 🏠 Architectural sketches to renderings\n",
        "\n",
        "                    ### Model Architecture:\n",
        "                    - **Input/Output**: 3-channel RGB images\n",
        "                    - **Generator**: U-Net with 8 encoder and 7 decoder layers\n",
        "                    - **Discriminator**: PatchGAN with 4 convolutional layers\n",
        "                    \"\"\"\n",
        "                )\n",
        "\n",
        "            with gr.Tab(\"🏋️ Training Guide\"):\n",
        "                gr.Markdown(\n",
        "                    \"\"\"\n",
        "                    ### Training Your Own Pix2Pix Model\n",
        "\n",
        "                    **1. Data Preparation:**\n",
        "                    ```python\n",
        "                    # Prepare paired training data (input-output image pairs)\n",
        "                    # Images should be aligned and of same size\n",
        "                    dataset_structure/\n",
        "                    ├── train/\n",
        "                    │   ├── A/  # Input images\n",
        "                    │   └── B/  # Target images\n",
        "                    └── test/\n",
        "                        ├── A/\n",
        "                        └── B/\n",
        "                    ```\n",
        "\n",
        "                    **2. Training Loop:**\n",
        "                    ```python\n",
        "                    # Loss functions\n",
        "                    criterion_GAN = nn.MSELoss()\n",
        "                    criterion_L1 = nn.L1Loss()\n",
        "                    lambda_L1 = 100  # L1 loss weight\n",
        "\n",
        "                    # Training\n",
        "                    for epoch in range(num_epochs):\n",
        "                        for real_A, real_B in dataloader:\n",
        "                            # Train Generator\n",
        "                            fake_B = generator(real_A)\n",
        "                            pred_fake = discriminator(real_A, fake_B)\n",
        "                            loss_GAN = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
        "                            loss_L1 = criterion_L1(fake_B, real_B)\n",
        "                            loss_G = loss_GAN + lambda_L1 * loss_L1\n",
        "\n",
        "                            # Train Discriminator\n",
        "                            pred_real = discriminator(real_A, real_B)\n",
        "                            pred_fake = discriminator(real_A, fake_B.detach())\n",
        "                            loss_D = (criterion_GAN(pred_real, torch.ones_like(pred_real)) +\n",
        "                                     criterion_GAN(pred_fake, torch.zeros_like(pred_fake))) * 0.5\n",
        "                    ```\n",
        "\n",
        "                    **3. Training Tips:**\n",
        "                    - Use paired training data for best results\n",
        "                    - Train for 100-200 epochs typically\n",
        "                    - Use learning rate scheduling\n",
        "                    - Monitor both G and D losses\n",
        "                    - Save model checkpoints regularly\n",
        "\n",
        "                    **4. Save Your Model:**\n",
        "                    ```python\n",
        "                    torch.save(generator.state_dict(), 'pix2pix_generator.pth')\n",
        "                    ```\n",
        "                    \"\"\"\n",
        "                )\n",
        "\n",
        "            with gr.Tab(\"📝 Examples\"):\n",
        "                gr.Markdown(\n",
        "                    \"\"\"\n",
        "                    ### Popular Pix2Pix Applications\n",
        "\n",
        "                    **🎨 Sketch to Photo:**\n",
        "                    - Input: Hand-drawn sketches or edge maps\n",
        "                    - Output: Photorealistic images\n",
        "                    - Use case: Art creation, concept visualization\n",
        "\n",
        "                    **🌆 Day to Night:**\n",
        "                    - Input: Daytime cityscape photos\n",
        "                    - Output: Nighttime scenes with lighting\n",
        "                    - Use case: Architectural visualization, film production\n",
        "\n",
        "                    **🛰️ Satellite to Map:**\n",
        "                    - Input: Satellite imagery\n",
        "                    - Output: Google Maps style images\n",
        "                    - Use case: Cartography, urban planning\n",
        "\n",
        "                    **🎨 Colorization:**\n",
        "                    - Input: Grayscale images\n",
        "                    - Output: Colorized versions\n",
        "                    - Use case: Historical photo restoration\n",
        "\n",
        "                    ### Tips for Best Results:\n",
        "                    - Use high-quality, well-aligned training pairs\n",
        "                    - Ensure consistent lighting and style in training data\n",
        "                    - Train for sufficient epochs (patience is key!)\n",
        "                    - Experiment with different loss weights\n",
        "                    \"\"\"\n",
        "                )\n",
        "\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            ---\n",
        "            💡 **Note**: This demo uses a randomly initialized model for demonstration.\n",
        "            Upload your own trained model for real transformations!\n",
        "            \"\"\",\n",
        "            elem_id=\"footer\"\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Initialize model\n",
        "generator = UNetGenerator(chnls_in=3, chnls_op=3)\n",
        "generator.eval()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_demo()\n",
        "    demo.launch(\n",
        "        share=True,  # Set to True for public sharing\n",
        "        debug=True,\n",
        "        show_error=True\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "9a65JlyFTZtS",
        "outputId": "81f86398-5890-40eb-b281-a7d3995fbe40"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://d331be2b6c1fbcadc5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d331be2b6c1fbcadc5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://d331be2b6c1fbcadc5.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8cW9QWm9TcKz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}